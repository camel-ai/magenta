"""
Custom Benchmark Example for Math Reasoning Library

å±•ç¤ºå¦‚ä½•åˆ›å»ºå’Œæ³¨å†Œè‡ªå®šä¹‰benchmark
"""

import os
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from math_reasoning_lib.core.pipeline import MathReasoningPipeline
from math_reasoning_lib.core.config import PipelineConfig
from math_reasoning_lib.core.base_classes import BaseBenchmark, MathProblem
from math_reasoning_lib.benchmarks.registry import register_benchmark


@dataclass
class CustomMathProblem(MathProblem):
    """è‡ªå®šä¹‰æ•°å­¦é—®é¢˜æ ¼å¼"""
    difficulty: str
    topic: str
    source: str
    hints: Optional[List[str]] = None


class CustomBenchmark(BaseBenchmark):
    """
    è‡ªå®šä¹‰benchmarkç¤ºä¾‹
    
    è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªæ–°çš„benchmarkç±»ï¼Œ
    å¯ä»¥åŠ è½½è‡ªå®šä¹‰æ ¼å¼çš„æ•°å­¦é—®é¢˜
    """
    
    def __init__(self, data_path: str = "custom_math_data.json"):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰benchmark
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data_path = data_path
        self.problems_cache = None
    
    def load_problems(
        self, 
        num_problems: int = 100, 
        difficulty: Optional[str] = None,
        topic: Optional[str] = None,
        **kwargs
    ) -> List[CustomMathProblem]:
        """
        åŠ è½½é—®é¢˜
        
        Args:
            num_problems: é—®é¢˜æ•°é‡
            difficulty: éš¾åº¦è¿‡æ»¤ (easy, medium, hard)
            topic: ä¸»é¢˜è¿‡æ»¤ (algebra, geometry, calculus, etc.)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            List[CustomMathProblem]: é—®é¢˜åˆ—è¡¨
        """
        if self.problems_cache is None:
            self._load_data()
        
        # è¿‡æ»¤é—®é¢˜
        filtered_problems = self.problems_cache
        
        if difficulty:
            filtered_problems = [
                p for p in filtered_problems 
                if p.difficulty.lower() == difficulty.lower()
            ]
        
        if topic:
            filtered_problems = [
                p for p in filtered_problems 
                if p.topic.lower() == topic.lower()
            ]
        
        # é™åˆ¶æ•°é‡
        return filtered_problems[:num_problems]
    
    def load_test_problems(
        self, 
        num_problems: int = 100, 
        **kwargs
    ) -> List[CustomMathProblem]:
        """
        åŠ è½½æµ‹è¯•é—®é¢˜
        
        Args:
            num_problems: é—®é¢˜æ•°é‡
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            List[CustomMathProblem]: æµ‹è¯•é—®é¢˜åˆ—è¡¨
        """
        # è¿™é‡Œå¯ä»¥åŠ è½½ä¸“é—¨çš„æµ‹è¯•é›†
        # ä¸ºäº†ç¤ºä¾‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒé›†çš„å­é›†
        all_problems = self.load_problems(num_problems * 2, **kwargs)
        return all_problems[num_problems:]  # ä½¿ç”¨ååŠéƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†
    
    def evaluate_solution(
        self, 
        problem: CustomMathProblem, 
        solution: str
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°è§£ç­”
        
        Args:
            problem: é—®é¢˜
            solution: è§£ç­”
            
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        # è¿™é‡Œå®ç°è‡ªå®šä¹‰çš„è¯„ä¼°é€»è¾‘
        # ä¾‹å¦‚ï¼šæ•°å€¼åŒ¹é…ã€ç¬¦å·åŒ¹é…ã€è¯­ä¹‰åŒ¹é…ç­‰
        
        # ç®€å•ç¤ºä¾‹ï¼šæ£€æŸ¥è§£ç­”ä¸­æ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆ
        is_correct = str(problem.answer).lower() in solution.lower()
        
        return {
            "correct": is_correct,
            "problem_id": problem.problem_id,
            "difficulty": problem.difficulty,
            "topic": problem.topic,
            "expected_answer": problem.answer,
            "solution_length": len(solution)
        }
    
    def get_metrics(self, evaluation_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: æ€§èƒ½æŒ‡æ ‡
        """
        if not evaluation_results:
            return {}
        
        total = len(evaluation_results)
        correct = sum(1 for r in evaluation_results if r["correct"])
        
        # æŒ‰éš¾åº¦åˆ†ç»„
        by_difficulty = {}
        for result in evaluation_results:
            diff = result["difficulty"]
            if diff not in by_difficulty:
                by_difficulty[diff] = {"total": 0, "correct": 0}
            by_difficulty[diff]["total"] += 1
            if result["correct"]:
                by_difficulty[diff]["correct"] += 1
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        by_topic = {}
        for result in evaluation_results:
            topic = result["topic"]
            if topic not in by_topic:
                by_topic[topic] = {"total": 0, "correct": 0}
            by_topic[topic]["total"] += 1
            if result["correct"]:
                by_topic[topic]["correct"] += 1
        
        return {
            "overall_accuracy": correct / total,
            "total_problems": total,
            "correct_answers": correct,
            "accuracy_by_difficulty": {
                diff: stats["correct"] / stats["total"] 
                for diff, stats in by_difficulty.items()
            },
            "accuracy_by_topic": {
                topic: stats["correct"] / stats["total"] 
                for topic, stats in by_topic.items()
            }
        }
    
    def _load_data(self):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        if not os.path.exists(self.data_path):
            # å¦‚æœæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
            self._create_sample_data()
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.problems_cache = []
        for item in data:
            problem = CustomMathProblem(
                problem_id=item["id"],
                problem_text=item["problem"],
                answer=item["answer"],
                difficulty=item["difficulty"],
                topic=item["topic"],
                source=item.get("source", "custom"),
                hints=item.get("hints", [])
            )
            self.problems_cache.append(problem)
    
    def _create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        sample_data = [
            {
                "id": "custom_001",
                "problem": "æ±‚è§£æ–¹ç¨‹ 2x + 5 = 13",
                "answer": "4",
                "difficulty": "easy",
                "topic": "algebra",
                "source": "custom_dataset",
                "hints": ["å°†å¸¸æ•°é¡¹ç§»åˆ°å³è¾¹", "é™¤ä»¥ç³»æ•°"]
            },
            {
                "id": "custom_002", 
                "problem": "è®¡ç®—åœ†çš„é¢ç§¯ï¼Œå…¶ä¸­åŠå¾„ä¸º 5",
                "answer": "78.54",
                "difficulty": "easy",
                "topic": "geometry",
                "source": "custom_dataset",
                "hints": ["ä½¿ç”¨å…¬å¼ Ï€ * rÂ²"]
            },
            {
                "id": "custom_003",
                "problem": "æ±‚å‡½æ•° f(x) = xÂ² + 3x - 4 çš„æœ€å°å€¼",
                "answer": "-6.25",
                "difficulty": "medium",
                "topic": "calculus",
                "source": "custom_dataset",
                "hints": ["æ‰¾åˆ°å¯¼æ•°ä¸ºé›¶çš„ç‚¹", "ä½¿ç”¨äºŒæ¬¡å‡½æ•°çš„é¡¶ç‚¹å…¬å¼"]
            },
            {
                "id": "custom_004",
                "problem": "è§£ä¸ç­‰å¼ 3x - 7 > 2x + 1",
                "answer": "x > 8",
                "difficulty": "medium",
                "topic": "algebra", 
                "source": "custom_dataset",
                "hints": ["ç§»é¡¹åˆå¹¶åŒç±»é¡¹"]
            },
            {
                "id": "custom_005",
                "problem": "åœ¨ä¸‰è§’å½¢ABCä¸­ï¼Œå·²çŸ¥a=5, b=7, C=60Â°ï¼Œæ±‚è¾¹cçš„é•¿åº¦",
                "answer": "6.08",
                "difficulty": "hard",
                "topic": "trigonometry",
                "source": "custom_dataset",
                "hints": ["ä½¿ç”¨ä½™å¼¦å®šç†", "cÂ² = aÂ² + bÂ² - 2ab*cos(C)"]
            }
        ]
        
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)


def example_custom_benchmark():
    """ç¤ºä¾‹ï¼šä½¿ç”¨è‡ªå®šä¹‰benchmark"""
    print("=== è‡ªå®šä¹‰Benchmarkç¤ºä¾‹ ===")
    
    # 1. æ³¨å†Œè‡ªå®šä¹‰benchmark
    register_benchmark("custom", CustomBenchmark)
    print("âœ… è‡ªå®šä¹‰benchmarkå·²æ³¨å†Œ")
    
    # 2. åˆ›å»ºé…ç½®
    config = PipelineConfig()
    config.openai_api_key = os.getenv("OPENAI_API_KEY")
    
    # 3. åˆ›å»ºç®¡é“
    pipeline = MathReasoningPipeline(config)
    
    # 4. è¿è¡Œæ•°æ®ç”Ÿæˆ
    result = pipeline.run_data_generation(
        benchmark="custom",
        model="gpt-4o-mini", 
        num_problems=5,
        toolkits=["sympy"],
        difficulty="easy",
        topic="algebra"
    )
    
    print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ")
    print(f"   æˆåŠŸç‡: {result.success_rate:.2%}")
    print(f"   å¤„ç†é—®é¢˜: {result.num_problems}")
    
    return result


def example_custom_benchmark_full_pipeline():
    """ç¤ºä¾‹ï¼šè‡ªå®šä¹‰benchmarkçš„å®Œæ•´ç®¡é“"""
    print("=== è‡ªå®šä¹‰Benchmarkå®Œæ•´ç®¡é“ ===")
    
    # æ³¨å†Œbenchmark
    register_benchmark("custom", CustomBenchmark)
    
    # åˆ›å»ºé…ç½®
    config = PipelineConfig()
    config.openai_api_key = os.getenv("OPENAI_API_KEY")
    
    # è°ƒæ•´è®­ç»ƒé…ç½®ä»¥é€‚åº”å°æ•°æ®é›†
    config.training_config.epochs = 1
    config.training_config.batch_size = 1
    
    # åˆ›å»ºç®¡é“
    pipeline = MathReasoningPipeline(config)
    
    # è¿è¡Œå®Œæ•´ç®¡é“
    results = pipeline.run_full_pipeline(
        benchmark="custom",
        base_model="gpt-4o-mini",
        num_problems=3,
        toolkits=["sympy"],
        difficulty="easy"
    )
    
    print("âœ… å®Œæ•´ç®¡é“æ‰§è¡Œå®Œæˆ")
    for result in results:
        print(f"   {result.stage}: {result.success_rate:.2%}")
    
    return results


def example_benchmark_comparison():
    """ç¤ºä¾‹ï¼šæ¯”è¾ƒä¸åŒbenchmarkçš„æ€§èƒ½"""
    print("=== Benchmarkæ¯”è¾ƒç¤ºä¾‹ ===")
    
    # æ³¨å†Œè‡ªå®šä¹‰benchmark
    register_benchmark("custom", CustomBenchmark)
    
    benchmarks = ["custom"]  # å¯ä»¥æ·»åŠ æ›´å¤šbenchmark: ["custom", "math", "gsm8k"]
    models = ["gpt-4o-mini"]
    
    results = {}
    
    for benchmark in benchmarks:
        results[benchmark] = {}
        
        for model in models:
            print(f"è¿è¡Œ {benchmark} with {model}")
            
            config = PipelineConfig()
            config.openai_api_key = os.getenv("OPENAI_API_KEY")
            
            pipeline = MathReasoningPipeline(config)
            
            result = pipeline.run_data_generation(
                benchmark=benchmark,
                model=model,
                num_problems=5,
                toolkits=["sympy"]
            )
            
            results[benchmark][model] = {
                "success_rate": result.success_rate,
                "num_problems": result.num_problems,
                "metrics": result.metrics
            }
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print("\nğŸ“Š æ¯”è¾ƒç»“æœ:")
    for benchmark, models_data in results.items():
        print(f"\n{benchmark.upper()}:")
        for model, data in models_data.items():
            print(f"  {model}: {data['success_rate']:.2%} ({data['num_problems']} é—®é¢˜)")


def main():
    """ä¸»å‡½æ•°"""
    print("è‡ªå®šä¹‰Benchmarkç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥API key
    if not os.getenv("OPENAI_API_KEY"):
        print("è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®: export OPENAI_API_KEY='your-api-key'")
        return
    
    try:
        # åŸºç¡€ç¤ºä¾‹
        example_custom_benchmark()
        print()
        
        # æ¯”è¾ƒç¤ºä¾‹
        example_benchmark_comparison()
        print()
        
        # å®Œæ•´ç®¡é“ç¤ºä¾‹ï¼ˆå¯é€‰ï¼Œéœ€è¦æ›´å¤šæ—¶é—´ï¼‰
        # example_custom_benchmark_full_pipeline()
        
        print("âœ… æ‰€æœ‰è‡ªå®šä¹‰benchmarkç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 