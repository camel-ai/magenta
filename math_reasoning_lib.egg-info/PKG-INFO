Metadata-Version: 2.4
Name: math_reasoning_lib
Version: 0.1.0
Summary: ç»Ÿä¸€çš„æ•°å­¦æ¨ç†åº“ï¼Œæ”¯æŒä¸åŒbenchmarkçš„ç«¯åˆ°ç«¯å¤„ç†
Author: Math Reasoning Team
Author-email: team@mathlib.com
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pyyaml>=6.0
Requires-Dist: dataclasses; python_version < "3.7"
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-asyncio; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Math Reasoning Library

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¨ç†åº“ï¼Œå°†åŸæœ‰çš„magentaé¡¹ç›®é‡æ„ä¸ºæ¨¡å—åŒ–çš„libraryï¼Œæ”¯æŒä¸åŒbenchmarkèµ°åŒä¸€ä¸ªpipelineï¼Œå¤§å¤§å‡å°‘é‡å¤æ€§å·¥ä½œã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”„ **ç»Ÿä¸€ç®¡é“**: å››é˜¶æ®µå·¥ä½œæµï¼ˆæ•°æ®ç”Ÿæˆ â†’ å¢å¼º â†’ è®­ç»ƒ â†’ è¯„ä¼°ï¼‰
- ğŸ“Š **å¤šBenchmarkæ”¯æŒ**: MATHã€GSM8Kã€AIMEç­‰ï¼Œæ˜“äºæ‰©å±•
- ğŸ› ï¸ **æ¨¡å—åŒ–è®¾è®¡**: ç»„ä»¶ç‹¬ç«‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- âš™ï¸ **çµæ´»é…ç½®**: æ”¯æŒYAML/JSONé…ç½®æ–‡ä»¶å’Œä»£ç é…ç½®
- ğŸ”Œ **æ’ä»¶ç³»ç»Ÿ**: åŠ¨æ€æ³¨å†Œbenchmarkã€æ¨¡å‹ã€å·¥å…·åŒ…
- ğŸ“ˆ **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šå®éªŒå¹¶è¡Œæ‰§è¡Œ

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
math_reasoning_lib/
â”œâ”€â”€ core/                          # æ ¸å¿ƒç®¡é“
â”‚   â”œâ”€â”€ pipeline.py               # ä¸»ç®¡é“ç±»
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ base_classes.py           # æŠ½è±¡åŸºç±»
â”œâ”€â”€ benchmarks/                    # åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ registry.py               # æ³¨å†Œå™¨
â”‚   â”œâ”€â”€ math_benchmark.py         # MATHæ•°æ®é›†
â”‚   â””â”€â”€ gsm8k_benchmark.py        # GSM8Kæ•°æ®é›†
â”œâ”€â”€ models/                        # æ¨¡å‹ç®¡ç†
â”œâ”€â”€ toolkits/                     # å·¥å…·åŒ…
â”œâ”€â”€ agents/                        # æ™ºèƒ½ä»£ç†
â”œâ”€â”€ enhancement/                   # æ•°æ®å¢å¼º
â”œâ”€â”€ training/                      # è®­ç»ƒæ¨¡å—
â”œâ”€â”€ evaluation/                    # è¯„ä¼°æ¨¡å—
â””â”€â”€ examples/                      # ä½¿ç”¨ç¤ºä¾‹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install -e .
```

### åŸºç¡€ä½¿ç”¨

```python
from math_reasoning_lib.core.pipeline import MathReasoningPipeline
from math_reasoning_lib.core.config import PipelineConfig, get_benchmark_config

# 1. åˆ›å»ºé…ç½®
config = PipelineConfig.from_dict(get_benchmark_config("math"))
config.openai_api_key = "your-api-key"

# 2. åˆ›å»ºç®¡é“
pipeline = MathReasoningPipeline(config)

# 3. è¿è¡Œå®Œæ•´ç®¡é“
results = pipeline.run_full_pipeline(
    benchmark="math",
    base_model="gpt-4o-mini",
    num_problems=100,
    toolkits=["sympy", "code_execution"]
)

# 4. æŸ¥çœ‹ç»“æœ
for result in results:
    print(f"{result.stage}: {result.success_rate:.2%}")
```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### 1. å•é˜¶æ®µè¿è¡Œ

```python
# åªè¿è¡Œæ•°æ®ç”Ÿæˆ
result = pipeline.run_data_generation(
    benchmark="gsm8k",
    model="gpt-4o-mini",
    num_problems=50,
    toolkits=["code_execution"]
)
```

### 2. å¤šBenchmarkæ¯”è¾ƒ

```python
benchmarks = ["math", "gsm8k"]
models = ["gpt-4o-mini", "gpt-3.5-turbo"]

for benchmark in benchmarks:
    for model in models:
        config = PipelineConfig.from_dict(get_benchmark_config(benchmark))
        pipeline = MathReasoningPipeline(config)
        
        result = pipeline.run_data_generation(
            benchmark=benchmark,
            model=model,
            num_problems=20
        )
        print(f"{benchmark}-{model}: {result.success_rate:.2%}")
```

### 3. è‡ªå®šä¹‰é…ç½®

```python
custom_config = {
    "solver": {
        "max_iterations": 20,
        "timeout": 900,
        "retry_attempts": 5
    },
    "training": {
        "epochs": 5,
        "batch_size": 2,
        "rank": 128
    }
}

config = PipelineConfig.from_dict(custom_config)
pipeline = MathReasoningPipeline(config)
```

### 4. é…ç½®æ–‡ä»¶

```yaml
# config.yaml
solver:
  max_iterations: 15
  timeout: 600
  multi_step: true

enhancement:
  max_retries: 3
  cot_generation: true

training:
  epochs: 3
  batch_size: 4
  rank: 64

openai_api_key: "your-api-key"
```

```python
config = PipelineConfig.from_file("config.yaml")
pipeline = MathReasoningPipeline(config)
```

## ğŸ”§ è‡ªå®šä¹‰Benchmark

### 1. åˆ›å»ºBenchmarkç±»

```python
from math_reasoning_lib.core.base_classes import BaseBenchmark, MathProblem

class CustomBenchmark(BaseBenchmark):
    def load_problems(self, num_problems=100, **kwargs):
        # å®ç°é—®é¢˜åŠ è½½é€»è¾‘
        return problems
    
    def load_test_problems(self, num_problems=100, **kwargs):
        # å®ç°æµ‹è¯•é—®é¢˜åŠ è½½é€»è¾‘
        return test_problems
    
    def evaluate_solution(self, problem, solution):
        # å®ç°è§£ç­”è¯„ä¼°é€»è¾‘
        return {"correct": True, "score": 1.0}
    
    def get_metrics(self, evaluation_results):
        # å®ç°æŒ‡æ ‡è®¡ç®—é€»è¾‘
        return {"accuracy": 0.85}
```

### 2. æ³¨å†Œå’Œä½¿ç”¨

```python
from math_reasoning_lib.benchmarks.registry import register_benchmark

# æ³¨å†Œè‡ªå®šä¹‰benchmark
register_benchmark("custom", CustomBenchmark)

# ä½¿ç”¨è‡ªå®šä¹‰benchmark
pipeline = MathReasoningPipeline(config)
result = pipeline.run_data_generation(
    benchmark="custom",
    model="gpt-4o-mini",
    num_problems=50
)
```

## ğŸ“Š æ”¯æŒçš„Benchmark

| Benchmark | æè¿° | é…ç½®æ¨¡æ¿ |
|-----------|------|----------|
| MATH | é«˜ä¸­æ•°å­¦ç«èµ›é¢˜ | `get_benchmark_config("math")` |
| GSM8K | å°å­¦æ•°å­¦åº”ç”¨é¢˜ | `get_benchmark_config("gsm8k")` |
| AIME | ç¾å›½æ•°å­¦é‚€è¯·èµ› | `get_benchmark_config("aime")` |
| Custom | è‡ªå®šä¹‰benchmark | éœ€è¦å®ç°BaseBenchmark |

## ğŸ› ï¸ æ”¯æŒçš„å·¥å…·åŒ…

- **SymPy Toolkit**: ç¬¦å·æ•°å­¦è®¡ç®—
- **Code Execution**: ä»£ç æ‰§è¡Œå’Œè®¡ç®—
- **Geometry Toolkit**: å‡ ä½•é—®é¢˜æ±‚è§£
- **Custom Toolkits**: è‡ªå®šä¹‰å·¥å…·åŒ…

## ğŸ›ï¸ é…ç½®é€‰é¡¹

### æ±‚è§£å™¨é…ç½®
```python
solver_config = {
    "max_iterations": 10,      # æœ€å¤§è¿­ä»£æ¬¡æ•°
    "timeout": 300,            # è¶…æ—¶æ—¶é—´(ç§’)
    "multi_step": True,        # å¤šæ­¥å¯¹è¯
    "enable_verification": True, # å¯ç”¨éªŒè¯
    "retry_attempts": 3        # é‡è¯•æ¬¡æ•°
}
```

### å¢å¼ºé…ç½®
```python
enhancement_config = {
    "max_retries": 3,          # æœ€å¤§é‡è¯•æ¬¡æ•°
    "enable_verification": True, # å¯ç”¨éªŒè¯
    "cot_generation": True,    # ç”ŸæˆCoTæ¨ç†
    "temperature": 0.1         # ç”Ÿæˆæ¸©åº¦
}
```

### è®­ç»ƒé…ç½®
```python
training_config = {
    "epochs": 3,               # è®­ç»ƒè½®æ•°
    "batch_size": 4,           # æ‰¹æ¬¡å¤§å°
    "learning_rate": 2e-4,     # å­¦ä¹ ç‡
    "rank": 64,                # LoRA rank
    "max_seq_length": 4096     # æœ€å¤§åºåˆ—é•¿åº¦
}
```

## ğŸ”„ å››é˜¶æ®µå·¥ä½œæµ

### é˜¶æ®µ1: æ•°æ®ç”Ÿæˆ (TIRè½¨è¿¹)
```python
result = pipeline.run_data_generation(
    benchmark="math",
    model="gpt-4o-mini",
    num_problems=100,
    toolkits=["sympy", "code_execution"]
)
```

### é˜¶æ®µ2: æ•°æ®å¢å¼º (Back-Translation)
```python
result = pipeline.run_enhancement(
    benchmark="math",
    enhancement_model="gpt-4o-mini"
)
```

### é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ (SFT)
```python
result = pipeline.run_training(
    base_model="Qwen/Qwen2.5-7B-Instruct",
    benchmark="math",
    training_config={"epochs": 3, "rank": 64}
)
```

### é˜¶æ®µ4: æ¨¡å‹è¯„ä¼°
```python
result = pipeline.run_evaluation(
    model_path="outputs/math_model",
    benchmark="math",
    num_problems=100
)
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### æŸ¥çœ‹ç»“æœ
```python
# è·å–æ‰€æœ‰ç»“æœ
results = pipeline.get_results()

# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
pipeline.save_results("experiment_results.json")

# æ‰“å°ç»“æœæ‘˜è¦
for result in results:
    print(f"é˜¶æ®µ: {result.stage}")
    print(f"æˆåŠŸç‡: {result.success_rate:.2%}")
    print(f"æŒ‡æ ‡: {result.metrics}")
```

### ç»“æœæ ¼å¼
```json
{
  "stage": "data_generation",
  "benchmark": "math",
  "model": "gpt-4o-mini",
  "num_problems": 100,
  "success_rate": 0.85,
  "metrics": {
    "solutions_generated": 85,
    "average_time": 45.2
  },
  "errors": []
}
```

## ğŸ”— å¹¶è¡Œå¤„ç†

```python
import concurrent.futures

def run_experiment(benchmark, model):
    config = PipelineConfig.from_dict(get_benchmark_config(benchmark))
    pipeline = MathReasoningPipeline(config)
    return pipeline.run_data_generation(
        benchmark=benchmark,
        model=model,
        num_problems=50
    )

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå®éªŒ
experiments = [("math", "gpt-4o-mini"), ("gsm8k", "gpt-4o-mini")]

with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
    futures = [
        executor.submit(run_experiment, benchmark, model) 
        for benchmark, model in experiments
    ]
    
    for future in concurrent.futures.as_completed(futures):
        result = future.result()
        print(f"å®éªŒå®Œæˆ: {result.success_rate:.2%}")
```

## ğŸ“š æ›´å¤šç¤ºä¾‹

- `examples/basic_usage.py` - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
- `examples/custom_benchmark.py` - è‡ªå®šä¹‰benchmarkç¤ºä¾‹
- `examples/advanced_pipeline.py` - é«˜çº§ç®¡é“ä½¿ç”¨

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **æ·»åŠ æ–°Benchmark**: ç»§æ‰¿`BaseBenchmark`ç±»
2. **æ·»åŠ æ–°æ¨¡å‹**: ç»§æ‰¿`BaseModel`ç±»  
3. **æ·»åŠ æ–°å·¥å…·åŒ…**: ç»§æ‰¿`BaseToolkit`ç±»
4. **æäº¤PR**: åŒ…å«æµ‹è¯•å’Œæ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨Apache 2.0è®¸å¯è¯ã€‚

## ğŸ†˜ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹examples/ç›®å½•ä¸‹çš„ç¤ºä¾‹
2. æŸ¥çœ‹APIæ–‡æ¡£
3. æäº¤GitHub Issue 
